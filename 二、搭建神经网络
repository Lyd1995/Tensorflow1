一、基于TensorFLow的神经网络（neural network，简称:NN）

（一）简介

用张量表示数据
用计算图搭建神经网络
用会话执行计算图
优化线上的权重（参数）
得出模型

（二）基本概念

1、张量（Tensor）：多维数组（列表）

阶：张量的维数

维数          阶            名字             例子
0-D           0         标量 scalar        s=123
1-D           1         向量 vector        v=[1,2,3]
2-D           2         矩阵 matrix        m=[[1,2,3],[4,5,6],[7,8,9]]
n-D           n         张量 tensor        t=[[[...(有n个中括号)

张量可以表示0阶到n阶数组（列表）

判断张量的阶数，只要看它的括号有几个，n个括号，阶数为n

（二）基本操作

1、数据类型
tf.float32    tf.int32


2、计算图

（1）引例：
以下代码描述的是一个计算过程，就是一张计算图
import tensorflow as tf
a=tf.constant([1.0,2.0])
b=tf.constant([3.0,4.0])

result=a+b
print result
输出：
Tensor("add:0", shape=(2,), dtype=float32)

这里输出的是有关result的信息
翻译：result是一个名为add:0的张量，维度为一，第一个维度里有2个元素，数据类型是float32型
这里没有显示出计算结果

"add:0"：
add：节点名
0：第0个输出

shape=(2,)
shape：维度
2：一维数组长度为2（如果shape的括号里有两个数，则表示维度是2）
可知：shape的括号里有n个数，则result的维度是n


dtype=float32
dtype:数据类型


（2）计算图与神经元
计算图（Graph）：搭建神经网络的计算过程，只搭建，不运算（只显示计算过程，不进行计算）
计算图描述的是一个神经网络的计算节点

这个计算图描述了一个神经元
import tensorflow as tf
x=tf.constant([[1.0,2.0]])
w=tf.constant([3.0],[4.0])

y=tf.matmul(x,w)
print y
Tensor("MatMul:0", shape=(1, 1), dtype=float32)
表示神经元：
y=X*W=x1*w1+x2*w2(矩阵运算)

注意：代码编写容易出错，首先X与W都应该是矩阵，也就是说这两个张量的阶是2，因此需要两个[]

3、会话（session）
会话：执行计算图中的节点运算

（1）引例：
import tensorflow as tf
x=tf.constant([[1.0,2.0]])
w=tf.constant([[3.0],[4.0]])

y=tf.matmul(x,w)
print y

with tf.Session() as sess: ## 不加with也能运行
	print sess.run(y)
输出：
Tensor("MatMul:0", shape=(1, 1), dtype=float32)
2018-09-15 02:52:41.335496: I tensorflow/core/platform/cpu_feature_guard.cc:141] 
Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[[11.]]

注意：此时运行程序出现了警告，大意：你没有启动加速运算
输入：
 vim ~/.bashrc
 在里面最后一行加上：
 export TF_CPP_MIN_LOG_LEVEL=2 //降低警告等价
 启动配置：
 source ~/.bashrc
 
 
    
with语句执行的解析：
　　with context_expr() as var:
　　　　doSomething()
当with语句执行时，便执行上下文表达式（context_expr）(一般为某个方法)来获得一个上下文管理器对象，
上下文管理器的职责是提供一个上下文对象，用于在with语句块中处理细节

一旦获得了上下文对象，就会调用它的__enter__()方法，将完成with语句块执行前的所有准备工作，
如果with语句后面跟了as语句，则用__enter__()方法的返回值来赋值；

当with语句块结束时，无论是正常结束，还是由于异常，都会调用上下文对象的__exit__()方法，__exit__()方法有3个参数，
如果with语句正常结束，三个参数全部都是 None；
如果发生异常，三个参数的值分别等于调用sys.exc_info()函数返回的三个值：类型（异常类）、值（异常实例）和跟踪记录（traceback），相应的跟踪记录对象。

因为上下文管理器主要作用于共享资源，__enter__()和__exit__()方法基本是完成的是分配和释放资源的低层次工作
比如：数据库连接、锁分配、信号量加/减、状态管理、文件打开/关闭、异常处理等。



4、参数

参数：即线上的权重W，用变量表示，随机赋予初值

w=tf.Variable(tf.random_normal([2,3], stddev=2, mean=0, seed=1))
tf.random_normal():正态分布，里面有四个参数
参数[2,3]:产生2*3的矩阵
参数stddev:设置标准差
参数mean:设置平均值
参数seed:s种子，不填写则采用当前时间

其他随机值：
tf.truncated_normal():去掉过大偏离点的正态分布
tf.random_uniform():平均分布

tf.zero 
tf.one
tf.fill  全部都为定值     tf.fill([2,3],6)  生成[[6,6,6],[6,6,6]]
tf.constant 直接给定值

二、搭建神经网络

（一）神经网络的实现过程
1、准备数据集，提取特征，作为输入喂给神经网络（neural network，NN）
2、搭建NN结构，从输入到输出（先搭建计算图，再使用会话执行）
            （NN前向传播算法----------->计算输出）
3、大量特征数据喂给NN，迭代优化NN参数
       （NN反向传播算法---------------->优化参数训练模型）
4、使用训练好的模型预测和分类

前三步（1，2，3）都是训练过程，第四步（4）是使用过程，一旦参数优化好了，就可以固定参数，拿去使用

（二）前向传播----->搭建模型，实现推理

前向传播----->搭建模型，实现推理
前向传播实际上是根据输入计算输出的过程

例如：生产一批零件，将体积x1和重量x2作为特征，输入到NN，通过NN后输出一个数值（这个过程就是前向传播）
